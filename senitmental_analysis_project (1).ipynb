{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d2V8k9BssMG",
        "outputId": "b27d65f5-0e13-4db2-f360-946447cd24c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# Importing the libraries\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# To count the iterations\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/Reviews.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhEHDLBftA6V",
        "outputId": "cae5e9dc-fc10-48f8-ff8f-03f34f7baa3b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-4eca766ad774>:5: DtypeWarning: Columns (1,2,3,8,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  dataset = pd.read_csv('/content/drive/MyDrive/Reviews.csv')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.head(60000)"
      ],
      "metadata": {
        "id": "SlO8wqpD_Eg-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dropping the dups in dataset\n",
        "dataset = dataset.drop_duplicates(subset=[\"UserId\", \"ProfileName\", \"Time\", \"Text\"], keep='first', inplace=False)\n",
        "\n",
        "def removeHTMLTags(review):\n",
        "    soup = BeautifulSoup(review, 'lxml')\n",
        "    return soup.get_text()\n",
        "\n",
        "def removeApostrophe(review):\n",
        "    phrase = re.sub(r\"won't\", \"will not\", review)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", review)\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", review)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", review)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", review)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", review)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", review)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", review)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", review)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", review)\n",
        "    return phrase\n",
        "\n",
        "def removeAlphaNumericWords(review):\n",
        "     return re.sub(\"\\S*\\d\\S*\", \"\", review).strip()\n",
        "\n",
        "def removeSpecialChars(review):\n",
        "     return re.sub('[^a-zA-Z]', ' ', review)\n",
        "\n",
        "def scorePartition(x):\n",
        "    if x < 3:\n",
        "        return 0\n",
        "    return 1\n",
        "\n",
        "def doTextCleaning(review):\n",
        "    review = removeHTMLTags(review)\n",
        "    review = removeApostrophe(review)\n",
        "    review = removeAlphaNumericWords(review)\n",
        "    review = removeSpecialChars(review)\n",
        "    # Lower casing\n",
        "    review = review.lower()\n",
        "    #Tokenization\n",
        "    review = review.split()\n",
        "    #Removing StopwordsÂ and Lemmatization\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "    review = [lmtzr.lemmatize(word, 'v') for word in review if not word in set(stopwords.words('english'))]\n",
        "    review = \" \".join(review)\n",
        "    return review\n",
        "\n"
      ],
      "metadata": {
        "id": "S1taxL2Bs_UL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generalizing the score\n",
        "actualScore = dataset['Score']\n",
        "positiveNegative = actualScore.map(scorePartition)\n",
        "dataset['Score'] = positiveNegative"
      ],
      "metadata": {
        "id": "rrgephngH1PD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the document corpus\n",
        "corpus = []\n",
        "for index, row in tqdm(dataset.iterrows()):\n",
        "    review = doTextCleaning(row['Text'])\n",
        "    corpus.append(review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrGyW9YEH2-7",
        "outputId": "d5e403f0-8e14-4747-eb1a-1dd6f1b06d6e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "79it [00:04, 53.31it/s]<ipython-input-5-5f5b05ee2511>:5: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(review, 'lxml')\n",
            "54804it [12:11, 74.92it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Bag of Words model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#Creating a tranform\n",
        "cv = CountVectorizer(ngram_range=(1,3), max_features = 5000)\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "y = dataset.iloc[:,6].values\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
        "\n",
        "# Fitting Naive Bayes to the Training set\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)"
      ],
      "metadata": {
        "id": "ksOYl-hAH5E1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the sentiment for new review\n",
        "newReview = str(input(\"Type the Review: \"))\n",
        "if newReview =='':\n",
        "  print('Invalid Review')\n",
        "else:\n",
        "  newReview = doTextCleaning(newReview)\n",
        "  new_review = cv.transform([newReview]).toarray()\n",
        "  prediction =  classifier.predict(new_review)\n",
        "if prediction[0] == 1:\n",
        "  print( \"Positive Review\" )\n",
        "else:\n",
        "  print( \"Negative Review\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqF4EZQZK0sF",
        "outputId": "a2a11110-54be-4550-91c3-9ac6ec93ed30"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type the Review: I will not buy this product.\n",
            "Negative Review\n"
          ]
        }
      ]
    }
  ]
}